# -*- coding: utf-8 -*-
"""Deep Learning Fashion MNIST Project 2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1imBPIX9wXDVVIMMzRdoaClbOcOrWhAg8

# Deep Learning Fashion MNIST Project 2025

**Author:** Muhammad Mubashar Shahzad  
**Dataset:** Fashion MNIST (Classification, 10 Classes)  
**Objective:**  
- Apply classic and deep learning models on Fashion MNIST
- Compare Logistic Regression, Dense Neural Network, and Convolutional Neural Network (CNN)
- Highlight improvement of CNN over baselines

---

## 1. Load Libraries & Data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from keras.utils import to_categorical

# Load Fashion MNIST using keras.datasets
from keras.datasets import fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Show dataset shape
print(f"Train set: {X_train.shape}, {y_train.shape}")
print(f"Test set:  {X_test.shape}, {y_test.shape}")

"""## 2. Data Preprocessing

- Normalize pixel values to [0, 1]
- Prepare different formats for ML models (flattened) and CNN (with channel dimension)
- One-hot encode labels for neural networks

"""

# Flatten images for ML and Dense NN (784,)
X_train_flat = X_train.reshape(-1, 28*28) / 255.0
X_test_flat = X_test.reshape(-1, 28*28) / 255.0

# For CNN: add channel dimension (28,28,1)
X_train_cnn = (X_train / 255.0).reshape(-1, 28, 28, 1)
X_test_cnn = (X_test / 255.0).reshape(-1, 28, 28, 1)

# One-hot encode labels for NN/CNN
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

"""### Fashion MNIST Class Labels"""

class_names = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]
df_classes = pd.DataFrame({"Label": list(range(10)), "Class Name": class_names})
print(df_classes)
plt.figure(figsize=(10,5))
for i in range(10):
    plt.subplot(2, 5, i+1)
    idx = (y_train == i).nonzero()[0][0]
    plt.imshow(X_train[idx], cmap="gray")
    plt.title(class_names[i])
    plt.axis('off')
plt.tight_layout()
plt.show()

"""## 3. Baseline Model 1: Logistic Regression (Classical ML)

We start with a classic model that treats each image as a flat vector.

"""

print("----- Logistic Regression (Baseline ML) -----")
lr = LogisticRegression(max_iter=100, solver='saga', multi_class='multinomial', verbose=1)
lr.fit(X_train_flat, y_train)
y_pred_lr = lr.predict(X_test_flat)
lr_acc = accuracy_score(y_test, y_pred_lr)
print(f"Logistic Regression Test Accuracy: {lr_acc:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred_lr))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues')
plt.title("Logistic Regression Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""### Logistic Regression: Classification Report (Graphical)

"""

from sklearn.metrics import classification_report

# Classification report as dict
cr_lr = classification_report(y_test, y_pred_lr, output_dict=True)

# Prepare data for plotting
import pandas as pd
df_lr = pd.DataFrame(cr_lr).transpose().iloc[:10][['precision', 'recall', 'f1-score']]

df_lr.plot(kind='bar', figsize=(12,5))
plt.title("Logistic Regression - Classification Report by Class")
plt.ylim(0, 1.1)
plt.ylabel("Score")
plt.xlabel("Class Label")
plt.legend(loc='lower right')
plt.show()

"""## 4. Baseline Model 2: Dense Neural Network (Simple Deep Learning)

A feedforward neural network with two hidden layers, using all pixel values.

"""

print("----- Dense Neural Network (Baseline NN) -----")
dense_model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
dense_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
dense_model.fit(X_train_flat, y_train_cat, epochs=10, batch_size=128,
                validation_data=(X_test_flat, y_test_cat), verbose=2)
dense_loss, dense_acc = dense_model.evaluate(X_test_flat, y_test_cat, verbose=0)
print(f"Dense NN Test Accuracy: {dense_acc:.4f}")

"""### Dense Neural Network: Classification Report (Graphical)

"""

# Get predictions
y_pred_dense = dense_model.predict(X_test_flat).argmax(axis=1)
cr_dense = classification_report(y_test, y_pred_dense, output_dict=True)
df_dense = pd.DataFrame(cr_dense).transpose().iloc[:10][['precision', 'recall', 'f1-score']]

df_dense.plot(kind='bar', figsize=(12,5), color=['orange','coral','peru'])
plt.title("Dense NN - Classification Report by Class")
plt.ylim(0, 1.1)
plt.ylabel("Score")
plt.xlabel("Class Label")
plt.legend(loc='lower right')
plt.show()

"""## 5. Deep Learning Model: Convolutional Neural Network (CNN)

CNNs can learn spatial patterns and improve performance on images.

"""

print("----- Convolutional Neural Network (CNN) -----")
cnn_model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(10, activation='softmax')
])
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


EPOCHS = 10

history_cnn = cnn_model.fit(
    X_train_cnn, y_train_cat,
    epochs=EPOCHS,
    batch_size=128,
    validation_data=(X_test_cnn, y_test_cat),
    verbose=2
)

cnn_loss, cnn_acc = cnn_model.evaluate(X_test_cnn, y_test_cat, verbose=0)
print(f"CNN Test Accuracy: {cnn_acc:.4f}")

# Classification report and confusion matrix
y_pred_cnn = cnn_model.predict(X_test_cnn).argmax(axis=1)
print("CNN Classification Report:\n", classification_report(y_test, y_pred_cnn))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred_cnn), annot=True, fmt='d', cmap='Greens')
plt.title("CNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""### Convolutional Neural Network: Classification Report (Graphical)

"""

cr_cnn = classification_report(y_test, y_pred_cnn, output_dict=True)
df_cnn = pd.DataFrame(cr_cnn).transpose().iloc[:10][['precision', 'recall', 'f1-score']]

df_cnn.plot(kind='bar', figsize=(12,5), color=['forestgreen','mediumseagreen','palegreen'])
plt.title("CNN - Classification Report by Class")
plt.ylim(0, 1.1)
plt.ylabel("Score")
plt.xlabel("Class Label")
plt.legend(loc='lower right')
plt.show()

"""## 8. Classification Report: Graphical Comparison

Visual comparison of Precision, Recall, and F1-score for each class across all models.

"""

# Get predictions for each model
y_pred_lr = lr.predict(X_test_flat)
y_pred_dense = dense_model.predict(X_test_flat).argmax(axis=1)
y_pred_cnn = cnn_model.predict(X_test_cnn).argmax(axis=1)

# Get classification reports as dicts
cr_lr = classification_report(y_test, y_pred_lr, output_dict=True)
cr_dense = classification_report(y_test, y_pred_dense, output_dict=True)
cr_cnn = classification_report(y_test, y_pred_cnn, output_dict=True)

# List of model names and their classification report dicts
model_crs = [
    ('Logistic Regression', cr_lr),
    ('Dense NN', cr_dense),
    ('CNN', cr_cnn)
]

classes = list(map(str, range(10)))  # Class names (0-9)
metrics = ['precision', 'recall', 'f1-score']

# Prepare data for plotting
x = np.arange(len(classes))  # the label locations
width = 0.22  # width of the bars

fig, axes = plt.subplots(1, 3, figsize=(20,6), sharey=True)
for i, metric in enumerate(metrics):
    ax = axes[i]
    for j, (name, cr) in enumerate(model_crs):
        scores = [cr[cls][metric] for cls in classes]
        ax.bar(x + j*width, scores, width=width, label=name)
    ax.set_xticks(x + width)
    ax.set_xticklabels(classes)
    ax.set_ylim(0, 1.1)
    ax.set_xlabel("Class")
    ax.set_ylabel(metric.capitalize())
    ax.set_title(f"{metric.capitalize()} by Model and Class")
    ax.legend()

plt.suptitle("Classification Report Metrics for Each Model")
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""## 6. Comparison of All Models

Let's compare all models side by side.

"""

# Prepare results for comparison
model_names = ['Logistic Regression', 'Dense NN', 'CNN']
model_scores = [lr_acc, dense_acc, cnn_acc]

# Print comparison table
print("\n======= Model Accuracy Comparison =======")
for name, score in zip(model_names, model_scores):
    print(f"{name:22}: {score:.4f}")

# Bar plot for visual comparison
plt.figure(figsize=(7,4))
sns.barplot(x=model_names, y=model_scores)
plt.ylim(0.5, 1.0)
plt.ylabel('Test Accuracy')
plt.title('Comparison of Model Accuracies on Fashion MNIST')
plt.show()

"""## 9. Hyperparameter Tuning for CNN (Keras Tuner)

To further improve performance, we apply automated hyperparameter tuning to our CNN using [Keras Tuner](https://keras.io/keras_tuner/).  
We search for the best combination of filters, kernel sizes, dense units, and dropout rate.

- **Tuned hyperparameters:**  
  - Number of filters and kernel size in Conv2D layers
  - Number of dense units
  - Dropout rate

The goal is to maximize validation accuracy and achieve the most effective CNN for Fashion MNIST.

"""

!pip install keras-tuner --quiet
import keras_tuner as kt
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

def build_model(hp):
    model = Sequential()
    # First Conv2D layer
    model.add(Conv2D(
        filters=hp.Choice('conv_1_filters', values=[32, 64, 128]),
        kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),
        activation='relu',
        input_shape=(28, 28, 1)
    ))
    model.add(MaxPooling2D((2,2)))
    # Second Conv2D layer
    model.add(Conv2D(
        filters=hp.Choice('conv_2_filters', values=[32, 64, 128]),
        kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]),
        activation='relu'
    ))
    model.add(MaxPooling2D((2,2)))
    model.add(Flatten())
    # Dense layer
    model.add(Dense(
        hp.Choice('dense_units', values=[64, 128, 256]),
        activation='relu'
    ))
    model.add(Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1)))
    model.add(Dense(10, activation='softmax'))
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

"""### Running the Hyperparameter Search

We use random search for hyperparameters, testing several model configurations and choosing the one with the best validation accuracy.

"""

tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=1,
    directory='kt_dir',
    project_name='fashion_mnist'
)

# Run the hyperparameter search
tuner.search(X_train_cnn, y_train_cat, epochs=5, validation_split=0.2)

"""### Best Hyperparameters and Final Model Performance

We report the best hyperparameter configuration found and evaluate the tuned model on the test set.

"""

# Show the best hyperparameters
best_hp = tuner.get_best_hyperparameters(1)[0]
print("Best Hyperparameters Found:")
for k in best_hp.values.keys():
    print(f"{k}: {best_hp.get(k)}")

# Build the best model and train it
best_model = tuner.hypermodel.build(best_hp)
best_model.fit(X_train_cnn, y_train_cat, epochs=10, validation_data=(X_test_cnn, y_test_cat))

# Evaluate the best tuned model
loss, acc = best_model.evaluate(X_test_cnn, y_test_cat)
print(f"Best Tuned CNN Test Accuracy: {acc:.4f}")

"""# Visualize the Best Hyperparameters Found
Bar plot of best values picked by Keras Tuner:
"""

best_values = {
    "Conv2D Filters (L1)": best_hp.get("conv_1_filters"),
    "Conv2D Kernel Size (L1)": best_hp.get("conv_1_kernel"),
    "Conv2D Filters (L2)": best_hp.get("conv_2_filters"),
    "Conv2D Kernel Size (L2)": best_hp.get("conv_2_kernel"),
    "Dense Units": best_hp.get("dense_units"),
    "Dropout Rate": best_hp.get("dropout")
}

plt.figure(figsize=(8,4))
plt.bar(best_values.keys(), list(map(float, best_values.values())), color='seagreen')
plt.title("Best Hyperparameters Selected by Keras Tuner")
plt.ylabel("Value")
plt.xticks(rotation=30, ha='right')
plt.tight_layout()
plt.show()

"""### Interpretation

- The best tuned CNN hyperparameters and final accuracy are displayed above.
- Hyperparameter tuning can significantly improve model performance and reliability.
- For more robust results, increase the number of trials and epochs, or try [Bayesian Optimization](https://keras.io/keras_tuner/#bayesianoptimization) in Keras Tuner.

### Evaluation: Tuned CNN vs. Default CNN

We now evaluate the best CNN model found through hyperparameter tuning and compare its performance to our original (default) CNN model.
"""

# Evaluate the best tuned CNN on the test set
loss_tuned, acc_tuned = best_model.evaluate(X_test_cnn, y_test_cat)
print(f"Best Tuned CNN Test Accuracy: {acc_tuned:.4f}")

# Compare with the default/original CNN (previously computed: cnn_acc)
print(f"Original CNN Test Accuracy:  {cnn_acc:.4f}")

"""#### Comparison Table: Default vs. Tuned CNN

Below is a direct comparison of test accuracy for both models:

"""

# Simple comparison table
import pandas as pd

comparison_df = pd.DataFrame({
    "Model": ["Default CNN", "Tuned CNN"],
    "Test Accuracy": [cnn_acc, acc_tuned]
})
print(comparison_df)

# Bar plot for visual comparison
plt.figure(figsize=(6,4))
sns.barplot(x="Model", y="Test Accuracy", data=comparison_df, palette="viridis")
plt.ylim(0.5, 1.0)
plt.title("Default vs. Tuned CNN Test Accuracy")
plt.show()

"""## 10. Overall Conclusion

In this project, we systematically explored image classification on the Fashion MNIST dataset using three different approaches:

- **Logistic Regression (Classical Machine Learning):**  
  Treated each image as a flat vector of pixels. Achieved moderate accuracy, but struggled to capture spatial and hierarchical patterns in image data.

- **Dense Neural Network (Simple Deep Learning):**  
  Introduced hidden layers and non-linear activation functions, resulting in noticeable performance improvement over Logistic Regression. However, this approach still ignored spatial structure by flattening the images.

- **Convolutional Neural Network (Default):**  
  Leveraged convolutional and pooling layers to automatically learn spatial features and local patterns in images. This led to a significant increase in test accuracy, confirming the strength of CNNs for computer vision tasks.

- **Tuned CNN (with Keras Tuner):**  
  By systematically searching for the best hyperparameters, we achieved our highest test accuracy. Hyperparameter tuning allowed the model to better fit the data and generalize, further boosting performance.

### **Key Takeaways:**

- **Deep learning (especially CNNs) dramatically outperforms classical ML and basic neural nets for image classification,** thanks to its ability to extract spatial features.
- **Hyperparameter tuning** is crucial for getting the most out of deep neural networks, leading to optimal accuracy.
- **Visualization and metric comparison** across all models made it clear how each modeling choice impacted performance.

### **Final Recommendation:**
For image classification tasks like Fashion MNIST, **well-tuned CNNs should be the default choice**, while classical approaches are useful for benchmarking and understanding model strengths and weaknesses.

---

**This workflow demonstrates a modern, professional approach to machine learning:  
Baseline → Deep Learning → Hyperparameter Tuning → Careful Evaluation and Comparison.**

"""

